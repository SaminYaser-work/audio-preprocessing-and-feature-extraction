{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import statistics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.pipeline import Pipeline as IMBPipeline\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES5 = ['angry', 'happy', 'neutral', 'sad', 'surprise']\n",
    "CLASSES6 = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad']\n",
    "CLASSES7 = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "\n",
    "def load_data(ds, folder_name):\n",
    "    # ds = 'all_en'\n",
    "    isAug = False\n",
    "    # folder_name = 'yamnet_emb'\n",
    "    model_name = 'YAMNet'\n",
    "\n",
    "    if 1 == 2:\n",
    "        X_train = np.load(f'./features/{folder_name}/{ds}/train_aug_X.npy', allow_pickle=True)\n",
    "        y_train = np.load(f'./features/{folder_name}/{ds}/train_aug_y.npy', allow_pickle=True)\n",
    "    else:\n",
    "        X_train = np.load(f'./features/{folder_name}/{ds}/train_X.npy', allow_pickle=True)\n",
    "        y_train = np.load(f'./features/{folder_name}/{ds}/train_y.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "    X_val = np.load(f'./features/{folder_name}/{ds}/val_X.npy', allow_pickle=True)\n",
    "    y_val = np.load(f'./features/{folder_name}/{ds}/val_y.npy', allow_pickle=True)\n",
    "    X_test = np.load(f'./features/{folder_name}/{ds}/test_X.npy', allow_pickle=True)\n",
    "    y_test = np.load(f'./features/{folder_name}/{ds}/test_y.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "    if ds == 'crema':\n",
    "        classes = CLASSES6\n",
    "    elif ds == 'bser':\n",
    "        classes = CLASSES5\n",
    "    else: \n",
    "        classes = CLASSES7\n",
    "\n",
    "    # join train and val\n",
    "    X_train = np.concatenate((X_train, X_val), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "    X = np.concatenate((X_train, X_test), axis=0)\n",
    "    y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "    # print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "    # pca = PCA(n_components=0.70)\n",
    "\n",
    "    # X_train = pca.fit_transform(X_train)\n",
    "    # X_test = pca.transform(X_test)\n",
    "\n",
    "    # X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    # X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    # X_train.shape, X_test.shape\n",
    "\n",
    "    # class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    # # class_weights = dict(zip(np.unique(y_train), class_weights))\n",
    "    # class_weights = dict(enumerate(class_weights))\n",
    "    # sample_weights = class_weight.compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "    # sample_weights, len(sample_weights)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampling_count(y):\n",
    "    count = Counter(y)\n",
    "    harmonic_mean = round(statistics.harmonic_mean(count.values()))\n",
    "\n",
    "    us_sample_count = {}\n",
    "    os_sample_count = {}\n",
    "\n",
    "    for key, value in count.items():\n",
    "        if value > harmonic_mean:\n",
    "            us_sample_count[key] = harmonic_mean\n",
    "        elif value < harmonic_mean:\n",
    "            os_sample_count[key] = harmonic_mean\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "    return us_sample_count, os_sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test, classes, title ):\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    weighted_f1 = f1_score(y_test, preds, average='weighted')\n",
    "    macro_f1 = f1_score(y_test, preds, average='macro')\n",
    "    micro_f1 = f1_score(y_test, preds, average='micro')\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    print('F1 Scores')\n",
    "    print('---------')\n",
    "    print(f'Weighted: {weighted_f1}')\n",
    "    print(f'Macro: {macro_f1} (Equal weights to each class)')\n",
    "    print(f'Micro: {micro_f1}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "\n",
    "    # weighted_auc = roc_auc_score(y_test, preds, average='weighted', multi_class='ovr')\n",
    "    # macro_auc = roc_auc_score(y_test, preds, average='macro', multi_class='ovr')\n",
    "    # micro_auc = roc_auc_score(y_test, preds, average='micro', multi_class='ovr')\n",
    "    # print('AUC Scores')\n",
    "    # print('---------')\n",
    "    # print(f'Weighted: {weighted_auc}')\n",
    "    # print(f'Macro: {macro_auc} (Equal weights to each class)')\n",
    "    # print(f'Micro: {micro_auc}')\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "    print('Classification Report')\n",
    "    print('---------------------')\n",
    "    print(classification_report(y_test,preds, digits=4))\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "    print('Confusion Matrix')\n",
    "    print('----------------')\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "    cm = cm / cm.astype(float).sum(axis=1)\n",
    "\n",
    "    sns.set(font_scale=1.5)\n",
    "\n",
    "    fig, ax= plt.subplots(figsize=(15,8))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2g', ax=ax)\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix of ' + title); \n",
    "    ax.xaxis.set_ticklabels(classes)\n",
    "    ax.yaxis.set_ticklabels(classes)\n",
    "    plt.show()\n",
    "\n",
    "    return weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = load_data('all', 'vggish_emb')\n",
    "\n",
    "print(a.shape, b.shape)\n",
    "\n",
    "sns.countplot(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTETomek(sampling_strategy='all')\n",
    "\n",
    "X_res, y_res = smt.fit_resample(a, b)\n",
    "\n",
    "print(X_res.shape, y_res.shape)\n",
    "\n",
    "sns.countplot(y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dses = ['tess', 'ravdess', 'savee', 'crema', 'subesco', 'bser', 'all_bn', 'all_en', 'all']\n",
    "folder_names = ['yamnet_emb']\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        'name': 'Logistic Regression V1',\n",
    "        'model': LogisticRegression(multi_class='multinomial', solver='lbfgs', class_weight='balanced'),\n",
    "        'scale': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'Logistic Regression V2',\n",
    "        'model': LogisticRegression(max_iter = 500, tol=1e-5, C=1.5, multi_class='multinomial', solver='lbfgs', class_weight='balanced'),\n",
    "        'scale': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'Logistic Regression V3',\n",
    "        'model': LogisticRegression(max_iter = 1000, tol=1e-5,C=2, multi_class='multinomial', solver='lbfgs', class_weight='balanced'),\n",
    "        'scale': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'Logistic Regression V4',\n",
    "        'model': LogisticRegression(max_iter = 2000,tol=1e-5, C=2.5, multi_class='multinomial', solver='lbfgs', class_weight='balanced'),\n",
    "        'scale': True\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "scores = [\n",
    "    'accuracy',\n",
    "    'f1_weighted',\n",
    "]\n",
    "# metrics.get_scorer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IMBPipeline([\n",
    "    ('balance', SMOTETomek(sampling_strategy='all')),\n",
    "    ('clf', MLPClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'clf__hidden_layer_sizes': [(1024,), (512,), (2048,)],\n",
    "    'clf__learning_rate': ['adaptive', 'invscaling'],\n",
    "    'clf__learning_rate_init': [0.0001, 1e-5],\n",
    "    'clf__max_iter': [1000],\n",
    "    'clf__shuffle': [True],\n",
    "}\n",
    "\n",
    "scv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "grid = GridSearchCV(pipeline, params, cv=scv, scoring=scores, refit='f1_weighted', n_jobs=-1, verbose=1, return_train_score=True)\n",
    "\n",
    "X, y = load_data('all', 'vggish_emb')\n",
    "\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(grid.cv_results_)\n",
    "df = df.sort_values(by='mean_test_f1_weighted', ascending=False)\n",
    "df\n",
    "# df.to_csv('grid_search_results_savee.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reses = []\n",
    "# for folder_name in folder_names:\n",
    "#     for ds in dses:\n",
    "#         X, y = load_data(ds, folder_name)\n",
    "#         scv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "#         for model in models:\n",
    "#             print('Cross validating', model['name'], 'on', ds, 'with', folder_name)\n",
    "\n",
    "#             if model['scale']:\n",
    "#                 scaler = StandardScaler()\n",
    "#                 X = scaler.fit_transform(X)\n",
    "\n",
    "#             acc = []\n",
    "#             f1_weighted = []\n",
    "\n",
    "#             for train_index, test_index in scv.split(X, y):\n",
    "#                 X_train, X_test = X[train_index], X[test_index]\n",
    "#                 y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "#                 us_sample_count, os_sample_count = get_sampling_count(y_train)\n",
    "\n",
    "#                 # over = RandomOverSampler(sampling_strategy=os_sample_count)\n",
    "#                 # X_train, y_train = over.fit_resample(X_train, y_train)\n",
    "\n",
    "#                 # under = RandomUnderSampler(sampling_strategy=us_sample_count)\n",
    "#                 # X_train, y_train = under.fit_resample(X_train, y_train)\n",
    "\n",
    "#                 smt = SMOTETomek(sampling_strategy='all')\n",
    "#                 X_train, y_train = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "#                 m = model['model']\n",
    "#                 m.fit(X_train, y_train)\n",
    "\n",
    "#                 acc.append(accuracy_score(y_test, m.predict(X_test)))\n",
    "#                 f1_weighted.append(f1_score(y_test, m.predict(X_test), average='weighted'))\n",
    "\n",
    "#             reses.append({\n",
    "#                 'ds': ds,\n",
    "#                 'model': model['name'],\n",
    "#                 'folder': folder_name,\n",
    "#                 'weighted_f1': np.mean(f1_weighted),\n",
    "#                 'accuracy': np.mean(acc),\n",
    "#             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('results.pkl', 'wb') as f:\n",
    "#     pickle.dump(reses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('grid_search_results.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "df_dict = {\n",
    "    'model': [],\n",
    "    'dataset': [],\n",
    "    'folder': [],\n",
    "    'accuracy': [],\n",
    "    'f1_weighted': [],\n",
    "    'train_time': []\n",
    "}\n",
    "\n",
    "for res in reses:\n",
    "    df_dict['model'].append(res['model'])\n",
    "    df_dict['dataset'].append(res['ds'])\n",
    "    df_dict['folder'].append(res['folder'])\n",
    "    df_dict['f1_weighted'].append(res['weighted_f1'])\n",
    "    df_dict['accuracy'].append(res['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(reses)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "plt.figure(figsize=(50,20))\n",
    "ax = sns.barplot(x='ds', y='weighted_f1', hue='model', data=df[df['folder'] == 'vggish_emb'])\n",
    "plt.title('Weighted F1 Scores for VGGish Embeddings')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "            ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "    if p.get_height() > 6:\n",
    "        p.set_color('red')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,20))\n",
    "ax = sns.barplot(x='ds', y='accuracy', hue='model', data=df[df['folder'] == 'vggish_emb'])\n",
    "plt.title('Accuracy of different models on different datasets with VGGish embeddings')\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "            ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,20))\n",
    "ax = sns.barplot(x='ds', y='weighted_f1', hue='model', data=df[df['folder'] == 'yamnet_emb'])\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "            ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,20))\n",
    "ax = sns.barplot(x='dataset', y='train_time', hue='model', data=df[df['folder'] == 'yamnet_emb'])\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.4f'), (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "            ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.076147382029735, 1: 0.8737273013540464, 2: 1.0043436293436294, 3: 1.03045308244615, 4: 0.9095942303489474, 5: 0.78960349079871, 6: 1.6827493261455526}\n",
      "(24972,)\n",
      "Training features shape: (24972, 128)\n",
      "Training labels shape: (24972, 7)\n",
      "Validation features shape: (3536, 128)\n",
      "Validation labels shape: (3536, 7)\n",
      "Test features shape: (7085, 128)\n",
      "Test labels shape: (7085, 7)\n"
     ]
    }
   ],
   "source": [
    "isAug = False\n",
    "ds = 'all'\n",
    "folder_name = 'vggish_emb'\n",
    "\n",
    "if isAug:\n",
    "    train_features = np.load(f'./features/{folder_name}/{ds}/train_aug_X.npy', allow_pickle=True)\n",
    "    train_labels = np.load(f'./features/{folder_name}/{ds}/train_aug_y.npy', allow_pickle=True)\n",
    "else:\n",
    "    train_features = np.load(f'./features/{folder_name}/{ds}/train_X.npy', allow_pickle=True)\n",
    "    train_labels = np.load(f'./features/{folder_name}/{ds}/train_y.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "val_features = np.load(f'./features/{folder_name}/{ds}/val_X.npy', allow_pickle=True)\n",
    "val_labels = np.load(f'./features/{folder_name}/{ds}/val_y.npy', allow_pickle=True)\n",
    "test_features = np.load(f'./features/{folder_name}/{ds}/test_X.npy', allow_pickle=True)\n",
    "test_labels = np.load(f'./features/{folder_name}/{ds}/test_y.npy', allow_pickle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "\n",
    "labels = np.unique(train_labels)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=labels, y=train_labels)\n",
    "# class_weights = dict(zip(np.unique(y_train), class_weights))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "sample_weights = class_weight.compute_sample_weight(class_weight='balanced', y=train_labels)\n",
    "\n",
    "print(class_weights)\n",
    "print(sample_weights.shape)\n",
    "\n",
    "train_labels = train_labels.reshape(-1, 1)\n",
    "val_labels = val_labels.reshape(-1, 1)\n",
    "test_labels = test_labels.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "train_labels = encoder.fit_transform(train_labels).toarray()\n",
    "val_labels = encoder.transform(val_labels).toarray()\n",
    "test_labels = encoder.transform(test_labels).toarray()\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test features shape:', test_features.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "def plot_cm(labels, predictions):\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(15,8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2g\")\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "METRICS = [\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'), \n",
    "    # keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 256\n",
    "monitor = 'val_loss'\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor, \n",
    "    verbose=1,\n",
    "    patience=20,\n",
    "    mode='min',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "scheduling = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor,\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    cooldown=0,\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    min_delta=0.0001,\n",
    "    min_lr=1e-20\n",
    "    )\n",
    "\n",
    "def create_model(layers, dropout):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(128,)))\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        model.add(keras.layers.Dense(layers[i], activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dropout(dropout))\n",
    "    model.add(keras.layers.Dense(7, activation='softmax'))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    [2048],\n",
    "    [1024],\n",
    "    [512],\n",
    "    [256],\n",
    "    [128],\n",
    "    [512, 256],\n",
    "    [128, 256]\n",
    "]\n",
    "dropouts = [0, 0.2, 0.5]\n",
    "optimizers = [\n",
    "    tf.optimizers.Adam(learning_rate=1e-3),\n",
    "    tf.optimizers.Adamax(learning_rate=1e-3),\n",
    "    tf.optimizers.Adam(learning_rate=1e-4),\n",
    "    tf.optimizers.Adamax(learning_rate=1e-4),\n",
    "    tf.optimizers.Adam(learning_rate=1e-5),\n",
    "    tf.optimizers.Adamax(learning_rate=1e-5),\n",
    "]\n",
    "dses = ['tess', 'ravdess', 'savee', 'crema', 'subesco', 'bser', 'all_bn', 'all_en', 'all']\n",
    "folder_names = ['vggish_emb']\n",
    "isAugs = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2268,\n",
       " ([2048],\n",
       "  0,\n",
       "  <keras.optimizers.optimizer_v2.adam.Adam at 0x7f2745f86ec0>,\n",
       "  'tess',\n",
       "  'vggish_emb',\n",
       "  True))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "combinations = list(itertools.product(layers, dropouts, optimizers, dses, folder_names, isAugs))\n",
    "\n",
    "len(combinations), combinations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2268 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([2048], 0, <keras.optimizers.optimizer_v2.adam.Adam object at 0x7f2745f86ec0>, 'tess', 'vggish_emb', True)\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Epoch 27: early stopping\n",
      "4/4 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2268 [00:56<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/sam/Documents/Projects/Thesis/audio-preprocessing-and-feature-extraction/notebook/yamnet_v3.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/Documents/Projects/Thesis/audio-preprocessing-and-feature-extraction/notebook/yamnet_v3.ipynb#X42sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m model\u001b[39m.\u001b[39mfit(train_features, train_labels, validation_data\u001b[39m=\u001b[39m(val_features, val_labels), epochs\u001b[39m=\u001b[39mEPOCHS, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, callbacks\u001b[39m=\u001b[39m[early_stopping, scheduling], verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/Documents/Projects/Thesis/audio-preprocessing-and-feature-extraction/notebook/yamnet_v3.ipynb#X42sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test_features, batch_size\u001b[39m=\u001b[39mBATCH_SIZE)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/Documents/Projects/Thesis/audio-preprocessing-and-feature-extraction/notebook/yamnet_v3.ipynb#X42sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m results\u001b[39m.\u001b[39mappend(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/Documents/Projects/Thesis/audio-preprocessing-and-feature-extraction/notebook/yamnet_v3.ipynb#X42sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     {\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/Documents/Projects/Thesis/audio-preprocessing-and-feature-extraction/notebook/yamnet_v3.ipynb#X42sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minfo\u001b[39m\u001b[39m'\u001b[39m: comb,\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sam/Documents/Projects/Thesis/audio-preprocessing-and-feature-extraction/notebook/yamnet_v3.ipynb#X42sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mf1_weighted\u001b[39m\u001b[39m'\u001b[39m: f1_score(test_labels, predictions, average\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mweighted\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/Documents/Projects/Thesis/audio-preprocessing-and-feature-extraction/notebook/yamnet_v3.ipynb#X42sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m: accuracy_score(test_labels, predictions),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/Documents/Projects/Thesis/audio-preprocessing-and-feature-extraction/notebook/yamnet_v3.ipynb#X42sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcm\u001b[39m\u001b[39m'\u001b[39m: confusion_matrix(test_labels, predictions, normalize\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/Documents/Projects/Thesis/audio-preprocessing-and-feature-extraction/notebook/yamnet_v3.ipynb#X42sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mreport\u001b[39m\u001b[39m'\u001b[39m: classification_report(test_labels, predictions, digits\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/Documents/Projects/Thesis/audio-preprocessing-and-feature-extraction/notebook/yamnet_v3.ipynb#X42sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/Documents/Projects/Thesis/audio-preprocessing-and-feature-extraction/notebook/yamnet_v3.ipynb#X42sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1132\u001b[0m, in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf1_score\u001b[39m(\n\u001b[1;32m    998\u001b[0m     y_true,\n\u001b[1;32m    999\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1006\u001b[0m ):\n\u001b[1;32m   1007\u001b[0m     \u001b[39m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \n\u001b[1;32m   1009\u001b[0m \u001b[39m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[39m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1132\u001b[0m     \u001b[39mreturn\u001b[39;00m fbeta_score(\n\u001b[1;32m   1133\u001b[0m         y_true,\n\u001b[1;32m   1134\u001b[0m         y_pred,\n\u001b[1;32m   1135\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1136\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1137\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1138\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1139\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1140\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1141\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1270\u001b[0m, in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfbeta_score\u001b[39m(\n\u001b[1;32m   1145\u001b[0m     y_true,\n\u001b[1;32m   1146\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1154\u001b[0m ):\n\u001b[1;32m   1155\u001b[0m     \u001b[39m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \n\u001b[1;32m   1157\u001b[0m \u001b[39m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[39m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1270\u001b[0m     _, _, f, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m   1271\u001b[0m         y_true,\n\u001b[1;32m   1272\u001b[0m         y_pred,\n\u001b[1;32m   1273\u001b[0m         beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[1;32m   1274\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1275\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1276\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1277\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mf-score\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[1;32m   1278\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1279\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1280\u001b[0m     )\n\u001b[1;32m   1281\u001b[0m     \u001b[39mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1556\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1555\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1556\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[1;32m   1558\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1357\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m average_options \u001b[39mand\u001b[39;00m average \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1355\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39maverage has to be one of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1357\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m   1358\u001b[0m \u001b[39m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[39m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m present_labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     95\u001b[0m             type_true, type_pred\n\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "for comb in tqdm(combinations):\n",
    "    print(comb)\n",
    "    layer, dropout, optimizer, ds, folder_name, isAug = comb\n",
    "\n",
    "    if isAug:\n",
    "        train_features = np.load(f'./features/{folder_name}/{ds}/train_aug_X.npy', allow_pickle=True)\n",
    "        train_labels = np.load(f'./features/{folder_name}/{ds}/train_aug_y.npy', allow_pickle=True)\n",
    "    else:\n",
    "        train_features = np.load(f'./features/{folder_name}/{ds}/train_X.npy', allow_pickle=True)\n",
    "        train_labels = np.load(f'./features/{folder_name}/{ds}/train_y.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "    val_features = np.load(f'./features/{folder_name}/{ds}/val_X.npy', allow_pickle=True)\n",
    "    val_labels = np.load(f'./features/{folder_name}/{ds}/val_y.npy', allow_pickle=True)\n",
    "    test_features = np.load(f'./features/{folder_name}/{ds}/test_X.npy', allow_pickle=True)\n",
    "    test_labels = np.load(f'./features/{folder_name}/{ds}/test_y.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "    labels = np.unique(train_labels)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    val_features = scaler.transform(val_features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "    train_labels = train_labels.reshape(-1, 1)\n",
    "    val_labels = val_labels.reshape(-1, 1)\n",
    "    test_labels = test_labels.reshape(-1, 1)\n",
    "    \n",
    "    encoder = OneHotEncoder()\n",
    "    train_labels = encoder.fit_transform(train_labels).toarray()\n",
    "    val_labels = encoder.transform(val_labels).toarray()\n",
    "    test_labels = encoder.transform(test_labels).toarray()\n",
    "\n",
    "\n",
    "    # Model creation\n",
    "    out = len(labels)\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(train_features.shape[-1],)))\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        model.add(keras.layers.Dense(layers[i][0], activation='relu'))\n",
    "    \n",
    "    if dropout != 0:\n",
    "        model.add(keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(keras.layers.Dense(out, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=METRICS\n",
    "    )\n",
    "    model.fit(train_features, train_labels, validation_data=(val_features, val_labels), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping, scheduling], verbose=0)\n",
    "\n",
    "    res = model.evaluate(test_features, test_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "    true = encoder.inverse_transform(test_labels)\n",
    "    preds = encoder.inverse_transform(test_predictions)\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            'info': comb,\n",
    "            'res': res,\n",
    "            'f1_weighted': f1_score(true, preds, average='weighted'),\n",
    "            'accuracy': accuracy_score(true, preds),\n",
    "            'cm': confusion_matrix(test_labels, predictions, normalize='true'),\n",
    "            'report': classification_report(test_labels, predictions, digits=4)\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, scheduling],\n",
    "    validation_data=(val_features, val_labels),\n",
    "    class_weight=class_weights,\n",
    "    sample_weight=sample_weights,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_predictions = model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions = model.predict(test_features, batch_size=BATCH_SIZE)\n",
    "\n",
    "plt.plot(hist.history['loss'], label='train loss')\n",
    "plt.plot(hist.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist.history['categorical_accuracy'], label='train accuracy')\n",
    "plt.plot(hist.history['val_categorical_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "res = model.evaluate(test_features, test_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "for name, value in zip(model.metrics_names, res):\n",
    "  print(name, ': ', value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "true = encoder.inverse_transform(test_labels)\n",
    "preds = encoder.inverse_transform(test_predictions)\n",
    "\n",
    "mlp_weighted_f1 = f1_score(true, preds, average='weighted')\n",
    "mlp_macro_f1 = f1_score(true, preds, average='macro')\n",
    "mlp_micro_f1 = f1_score(true, preds, average='micro')\n",
    "mlp_accuracy = accuracy_score(true, preds)\n",
    "print('F1 Scores')\n",
    "print('---------')\n",
    "print(f'Weighted: {mlp_weighted_f1}')\n",
    "print(f'Macro: {mlp_macro_f1} (Equal weights to each class)')\n",
    "print(f'Micro: {mlp_micro_f1}')\n",
    "print(f'Accuracy: {mlp_accuracy}')\n",
    "\n",
    "print()\n",
    "print('Accuracy Score')\n",
    "print('--------------')\n",
    "mlp_acc = accuracy_score(true, preds) * 100\n",
    "print('Accuracy: ', mlp_acc)\n",
    "\n",
    "print(classification_report(true, preds, digits=4))\n",
    "plot_cm(true, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = {\n",
    "    'SVM': svm_f1 * 100,\n",
    "    'Logistic Regression': lr_f1 * 100,\n",
    "    'MLP': mlp_weighted_f1 * 100\n",
    "}\n",
    "\n",
    "\n",
    "accuracies = {\n",
    "    'SVM': svm_test_acc,\n",
    "    'Logistic Regression': lrm_test_acc,\n",
    "    'MLP': mlp_acc\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.barplot(y=list(accuracies.values()), x=list(accuracies.keys()))\n",
    "plt.title('Accuracy Scores')\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.barplot(y=list(f1s.values()), x=list(f1s.keys()))\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)\n",
    "plt.title('Weighted F1 Scores')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
