{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Import Libraries\n","\n","Import necessary libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-09-09T06:05:17.051852Z","iopub.status.busy":"2022-09-09T06:05:17.051497Z","iopub.status.idle":"2022-09-09T06:05:17.067240Z","shell.execute_reply":"2022-09-09T06:05:17.065732Z","shell.execute_reply.started":"2022-09-09T06:05:17.051810Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-09-24 08:24:15.687843: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-09-24 08:24:15.954888: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2022-09-24 08:24:15.954904: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","2022-09-24 08:24:15.980855: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2022-09-24 08:24:18.532520: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2022-09-24 08:24:18.532615: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2022-09-24 08:24:18.532621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["import glob\n","import os\n","import librosa\n","import librosa.display\n","import time\n","import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import noisereduce as nr\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","import skimage.io\n","import re\n","import pathlib\n","from IPython.display import display, Image\n","# %matplotlib inline\n","from sklearn.model_selection import train_test_split\n","from pydub import AudioSegment\n","from pydub.effects import normalize\n","\n","from tensorflow.keras.layers import Input, Dense, Flatten\n","from tensorflow.keras import Model\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import Sequential\n","import tensorflow as tf\n","import tensorflow_io as tfio"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class CFG:\n","    wandb = True\n","    project = \"fake-speech-detection\"\n","    debug = False\n","    exp_name = \"v0\"\n","    comment = \"Conformer-128x80-cosine-no_aug-no_fc\"\n","\n","    # Use verbose=0 for silent, 1 for interactive\n","    verbose = 0\n","    display_plot = True\n","\n","    # Device for training\n","    device = None  # device is automatically selected\n","\n","    # Model & Backbone\n","    model_name = \"Conformer\"\n","\n","    # Seeding for reproducibility\n","    seed = 101\n","\n","    # Audio params\n","    sample_rate = 16000\n","    duration = 3.0 # duration in second\n","    audio_len = int(sample_rate * duration)\n","    normalize = True\n","\n","    # Spectrogram params\n","    spec_freq = 128 # freq axis\n","    n_fft = 2048\n","    # spec_time = 256 # time axis\n","    # hop_len = audio_len//(spec_time - 1) # non-overlap region\n","    hop_len = n_fft // 4 # non-overlap region\n","    spec_time = (n_fft + 1) // hop_len # time axis\n","    fmin = 20\n","    fmax = sample_rate//2 # max frequency\n","    spec_shape = [spec_time, spec_freq] # output spectrogram shape\n","    \n","    # Audio Augmentation\n","    timeshift_prob = 0.0\n","    gn_prob = 0.0\n","    \n","    # Spectrogram Augmentation\n","    time_mask = 20\n","    freq_mask = 10\n","    cutmix_prob = 0.0\n","    cutmix_alpha = 2.5\n","    mixup_prob = 0.0\n","    mixup_alpha = 2.5\n","\n","    # Batch Size & Epochs\n","    batch_size = 32\n","    drop_remainder = False\n","    epochs = 12\n","    steps_per_execution = None\n","\n","    # Loss & Optimizer & LR Scheduler\n","    loss = \"binary_crossentropy\"\n","    optimizer = \"Adam\"\n","    lr = 1e-4\n","    lr_schedule = \"cosine\"\n","\n","    # Augmentation\n","    augment = False\n","\n","    # Clip values to [0, 1]\n","    clip = False"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def random_int(shape=[], minval=0, maxval=1):\n","    return tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.int32)\n","\n","\n","def random_float(shape=[], minval=0.0, maxval=1.0):\n","    rnd = tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.float32)\n","    return rnd\n","\n","# Trim Audio to ignore silent part in the start and end\n","def TrimAudio(audio, epsilon=0.15):\n","    pos  = tfio.audio.trim(audio, axis=0, epsilon=epsilon)\n","    audio = audio[pos[0]:pos[1]]\n","    return audio\n","\n","# Crop or Pad audio to keep a fixed length\n","def CropOrPad(audio, target_len, pad_mode='constant'):\n","    audio_len = tf.shape(audio)[0]\n","    if audio_len < target_len: # if audio_len is smaller than target_len then use Padding\n","        diff_len = (target_len - audio_len)\n","        pad1 = random_int([], minval=0, maxval=diff_len) # select random location for padding\n","        pad2 = diff_len - pad1\n","        pad_len = [pad1, pad2]\n","        audio = tf.pad(audio, paddings=[pad_len], mode=pad_mode) # apply padding\n","    elif audio_len > target_len:  # if audio_len is larger than target_len then use Cropping\n","        diff_len = (audio_len - target_len)\n","        idx = tf.random.uniform([], 0, diff_len, dtype=tf.int32) # select random location for cropping\n","        audio = audio[idx: (idx + target_len)]\n","    audio = tf.reshape(audio, [target_len])\n","    return audio\n","\n","# Randomly shift audio -> any sound at <t> time may get shifted to <t+shift> time\n","def TimeShift(audio, prob=0.5):\n","    if random_float() < prob:\n","        shift = random_int(shape=[], minval=0, maxval=tf.shape(audio)[0])\n","        if random_float() < 0.5:\n","            shift = -shift\n","        audio = tf.roll(audio, shift, axis=0)\n","    return audio\n","\n","# Apply random noise to audio data\n","def GaussianNoise(audio, std=[0.0025, 0.025], prob=0.5):\n","    std = random_float([], std[0], std[1])\n","    if random_float() < prob:\n","        GN = tf.keras.layers.GaussianNoise(stddev=std)\n","        audio = GN(audio, training=True) # training=False don't apply noise to data\n","    return audio\n","\n","# Applies augmentation to Audio Signal\n","def AudioAug(audio):\n","    audio = TimeShift(audio, prob=CFG.timeshift_prob)\n","    audio = GaussianNoise(audio, prob=CFG.gn_prob)\n","    return audio\n","\n","def Normalize(data):\n","    MEAN = tf.math.reduce_mean(data)\n","    STD = tf.math.reduce_std(data)\n","    data = tf.math.divide_no_nan(data - MEAN, STD)\n","    return data\n","\n","# Randomly mask data in time and freq axis\n","def TimeFreqMask(spec, time_mask, freq_mask, prob=0.5):\n","    if random_float() < prob:\n","        spec = tfio.audio.freq_mask(spec, param=freq_mask)\n","        spec = tfio.audio.time_mask(spec, param=time_mask)\n","    return spec\n","\n","# Applies augmentation to Spectrogram\n","def SpecAug(spec):\n","    spec = TimeFreqMask(spec, time_mask=CFG.time_mask, freq_mask=CFG.freq_mask, prob=0.5)\n","    return spec\n","\n","# Compute MixUp Augmentation for Spectrogram\n","def get_mixup(alpha=0.2, prob=0.5):\n","    \"\"\"Apply Spectrogram-MixUp augmentaiton. Apply Mixup to one batch and its shifted version\"\"\"\n","    def mixup(specs, labels, alpha=alpha, prob=prob):\n","        if random_float() > prob:\n","            return specs, labels\n","\n","        spec_shape = tf.shape(specs)\n","        label_shape = tf.shape(labels)\n","\n","        beta = tfp.distributions.Beta(alpha, alpha) # select lambda from beta distribution\n","        lam = beta.sample(1)[0]\n","        \n","        # It's faster to roll the batch by one instead of shuffling it to create image pairs\n","        specs = lam * specs + (1 - lam) * tf.roll(specs, shift=1, axis=0) # mixup = [1, 2, 3]*lam + [3, 1, 2]*(1 - lam)\n","        labels = lam * labels + (1 - lam) * tf.roll(labels, shift=1, axis=0)\n","\n","        specs = tf.reshape(specs, spec_shape)\n","        labels = tf.reshape(labels, label_shape)\n","        return specs, labels\n","    return mixup\n","\n","\n","def get_cutmix(alpha, prob=0.5):\n","    \"\"\"Apply Spectrogram-CutMix augmentaiton which only cuts patch across time axis unline \n","    typical Computer-Vision CutMix. Apply CutMix to one batch and its shifted version.\n","    \"\"\"\n","    def cutmix(specs, labels, alpha=alpha, prob=prob):\n","        if random_float() > prob:\n","            return specs, labels\n","        spec_shape = tf.shape(specs)\n","        label_shape = tf.shape(labels)\n","        W = tf.cast(spec_shape[1], tf.int32)  # [batch, time, freq, channel]\n","\n","        # Lambda from beta distribution\n","        beta = tfp.distributions.Beta(alpha, alpha)\n","        lam = beta.sample(1)[0]\n","        \n","        # It's faster to roll the batch by one instead of shuffling it to create image pairs\n","        specs_rolled = tf.roll(specs, shift=1, axis=0) # specs->[1, 2, 3], specs_rolled->[3, 1, 2]\n","        labels_rolled = tf.roll(labels, shift=1, axis=0)\n","\n","        # Select random patch size\n","        r_x = random_int([], minval=0, maxval=W)\n","        r = 0.5 * tf.math.sqrt(1.0 - lam)\n","        r_w_half = tf.cast(r * tf.cast(W, tf.float32), tf.int32)\n","\n","        # Select random location in time axis\n","        x1 = tf.cast(tf.clip_by_value(r_x - r_w_half, 0, W), tf.int32)\n","        x2 = tf.cast(tf.clip_by_value(r_x + r_w_half, 0, W), tf.int32)\n","\n","        # outer-pad patch -> [0, 0, x, x, 0, 0]\n","        patch1 = specs[:, x1:x2, :, :]  # [batch, time, freq, channel]\n","        patch1 = tf.pad(\n","            patch1, [[0, 0], [x1, W - x2], [0, 0], [0, 0]])  # outer-pad\n","\n","        # inner-pad-patch -> [y, y, 0, 0, y, y]\n","        patch2 = specs_rolled[:, x1:x2, :, :]  # [batch, mel, time, channel]\n","        patch2 = tf.pad(\n","            patch2, [[0, 0], [x1, W - x2], [0, 0], [0, 0]])  # outer-pad\n","        patch2 = specs_rolled - patch2  # inner-pad-patch = img - outer-pad-patch\n","        \n","        # patch1 -> [0, 0, x, x, 0, 0], patch2 -> [y, y, 0, 0, y, y]\n","        # cutmix = (patch1 + patch2) -> [y, y, x, x, y, y]\n","        specs = patch1 + patch2  # cutmix img\n","\n","        # Compute lambda = [1 - (patch_area/image_area)]\n","        lam = tf.cast((1.0 - (x2 - x1) / (W)),tf.float32)  # no H term as (y1 - y2) = H\n","        labels = lam * labels + (1.0 - lam) * labels_rolled  # cutmix label\n","\n","        specs = tf.reshape(specs, spec_shape)\n","        labels = tf.reshape(labels, label_shape)\n","\n","        return specs, labels\n","    return cutmix"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def stft(data):\n","    data = np.abs(librosa.stft(data, hop_length=HOP_LEGTH, n_fft=FRAME_SIZE))\n","    return librosa.power_to_db(data, ref=np.max)\n","\n","def chroma(data):\n","    data = stft(data)\n","    return librosa.feature.chroma_stft(S=stft, sr=RATE)\n","\n","def mel(data):\n","    mels = librosa.feature.melspectrogram(y=data, sr=RATE)\n","    return librosa.power_to_db(mels, ref=np.max)\n","\n","def mfcc(data):\n","    return librosa.feature.mfcc(y=data, sr=RATE, n_fft=FRAME_SIZE, hop_length=HOP_LEGTH)"]},{"cell_type":"markdown","metadata":{},"source":["# Building Dataframe"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-09-09T06:05:18.494794Z","iopub.status.busy":"2022-09-09T06:05:18.493909Z","iopub.status.idle":"2022-09-09T06:05:18.502347Z","shell.execute_reply":"2022-09-09T06:05:18.501192Z","shell.execute_reply.started":"2022-09-09T06:05:18.494746Z"},"trusted":true},"outputs":[],"source":["Crema_Path='../Datasets/Crema/'\n","Ravdess_Path='../Datasets/Ravdess/'\n","Savee_Path='../Datasets/Savee/'\n","Tess_Path='../Datasets/Tess/'"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-09-09T06:05:18.505263Z","iopub.status.busy":"2022-09-09T06:05:18.504001Z","iopub.status.idle":"2022-09-09T06:05:19.069001Z","shell.execute_reply":"2022-09-09T06:05:19.067988Z","shell.execute_reply.started":"2022-09-09T06:05:18.505222Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Emotion</th>\n","      <th>File_Path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>angry</td>\n","      <td>../Datasets/Crema//1072_IOM_ANG_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>fear</td>\n","      <td>../Datasets/Crema//1048_IOM_FEA_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>angry</td>\n","      <td>../Datasets/Crema//1073_IEO_ANG_LO.wav</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>happy</td>\n","      <td>../Datasets/Crema//1012_WSI_HAP_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>happy</td>\n","      <td>../Datasets/Crema//1073_TSI_HAP_XX.wav</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Emotion                               File_Path\n","0   angry  ../Datasets/Crema//1072_IOM_ANG_XX.wav\n","1    fear  ../Datasets/Crema//1048_IOM_FEA_XX.wav\n","2   angry  ../Datasets/Crema//1073_IEO_ANG_LO.wav\n","3   happy  ../Datasets/Crema//1012_WSI_HAP_XX.wav\n","4   happy  ../Datasets/Crema//1073_TSI_HAP_XX.wav"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["crema=[]\n","for wav in os.listdir(Crema_Path):\n","    emotion=wav.partition(\".wav\")[0].split('_')\n","    if emotion[2]=='SAD':\n","        crema.append(('sad',Crema_Path+'/'+wav))\n","    elif emotion[2]=='ANG':\n","        crema.append(('angry',Crema_Path+'/'+wav))\n","    elif emotion[2]=='DIS':\n","        crema.append(('disgust',Crema_Path+'/'+wav))\n","    elif emotion[2]=='FEA':\n","        crema.append(('fear',Crema_Path+'/'+wav))\n","    elif emotion[2]=='HAP':\n","        crema.append(('happy',Crema_Path+'/'+wav))\n","    elif emotion[2]=='NEU':\n","        crema.append(('neutral',Crema_Path+'/'+wav))\n","    else:\n","        crema.append(('unknown',Crema_Path+'/'+wav))\n","Crema_df=pd.DataFrame.from_dict(crema)\n","Crema_df.rename(columns={0:'Emotion',1:'File_Path'},inplace=True)\n","Crema_df.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-09-09T06:05:19.071114Z","iopub.status.busy":"2022-09-09T06:05:19.070808Z","iopub.status.idle":"2022-09-09T06:05:19.602929Z","shell.execute_reply":"2022-09-09T06:05:19.602060Z","shell.execute_reply.started":"2022-09-09T06:05:19.071075Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Emotion</th>\n","      <th>File_Path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>neutral</td>\n","      <td>../Datasets/Ravdess/Actor_08/03-01-02-01-01-01...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sad</td>\n","      <td>../Datasets/Ravdess/Actor_08/03-01-04-02-02-02...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>neutral</td>\n","      <td>../Datasets/Ravdess/Actor_08/03-01-02-01-02-01...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>happy</td>\n","      <td>../Datasets/Ravdess/Actor_08/03-01-03-01-02-01...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>fear</td>\n","      <td>../Datasets/Ravdess/Actor_08/03-01-06-01-02-02...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Emotion                                          File_Path\n","0  neutral  ../Datasets/Ravdess/Actor_08/03-01-02-01-01-01...\n","1      sad  ../Datasets/Ravdess/Actor_08/03-01-04-02-02-02...\n","2  neutral  ../Datasets/Ravdess/Actor_08/03-01-02-01-02-01...\n","3    happy  ../Datasets/Ravdess/Actor_08/03-01-03-01-02-01...\n","4     fear  ../Datasets/Ravdess/Actor_08/03-01-06-01-02-02..."]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["ravdess=[]\n","for directory in os.listdir(Ravdess_Path):\n","    actors=os.listdir(os.path.join(Ravdess_Path,directory))\n","    for wav in actors:\n","        emotion=wav.partition('.wav')[0].split('-')\n","        emotion_number=int(emotion[2])\n","        ravdess.append((emotion_number,os.path.join(Ravdess_Path,directory,wav)))\n","Ravdess_df=pd.DataFrame.from_dict(ravdess)\n","Ravdess_df.rename(columns={0:'Emotion',1:'File_Path'},inplace=True)\n","Ravdess_df['Emotion'].replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'},inplace=True)\n","Ravdess_df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-09-09T06:05:19.604848Z","iopub.status.busy":"2022-09-09T06:05:19.604311Z","iopub.status.idle":"2022-09-09T06:05:19.793114Z","shell.execute_reply":"2022-09-09T06:05:19.791845Z","shell.execute_reply.started":"2022-09-09T06:05:19.604803Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Emotion</th>\n","      <th>File_Path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>fear</td>\n","      <td>../Datasets/Savee//KL_f05.wav</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>disgust</td>\n","      <td>../Datasets/Savee//KL_d11.wav</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>neutral</td>\n","      <td>../Datasets/Savee//DC_n06.wav</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>disgust</td>\n","      <td>../Datasets/Savee//DC_d06.wav</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>surprise</td>\n","      <td>../Datasets/Savee//DC_su06.wav</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Emotion                       File_Path\n","0      fear   ../Datasets/Savee//KL_f05.wav\n","1   disgust   ../Datasets/Savee//KL_d11.wav\n","2   neutral   ../Datasets/Savee//DC_n06.wav\n","3   disgust   ../Datasets/Savee//DC_d06.wav\n","4  surprise  ../Datasets/Savee//DC_su06.wav"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["savee=[]\n","for wav in os.listdir(Savee_Path):\n","    emo=wav.partition('.wav')[0].split('_')[1].replace(r'[0-9]','')\n","    emotion=re.split(r'[0-9]',emo)[0]\n","    if emotion=='a':\n","        savee.append(('angry',Savee_Path+'/'+wav))\n","    elif emotion=='d':\n","        savee.append(('disgust',Savee_Path+'/'+wav))\n","    elif emotion=='f':\n","        savee.append(('fear',Savee_Path+'/'+wav))\n","    elif emotion=='h':\n","        savee.append(('happy',Savee_Path+'/'+wav))\n","    elif emotion=='n':\n","        savee.append(('neutral',Savee_Path+'/'+wav))\n","    elif emotion=='sa':\n","        savee.append(('sad',Savee_Path+'/'+wav))\n","    elif emotion=='su':\n","        savee.append(('surprise',Savee_Path+'/'+wav))\n","Savee_df=pd.DataFrame.from_dict(savee)\n","Savee_df.rename(columns={0:'Emotion',1:'File_Path'},inplace=True)\n","Savee_df.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-09-09T06:05:19.797920Z","iopub.status.busy":"2022-09-09T06:05:19.797339Z","iopub.status.idle":"2022-09-09T06:05:20.869325Z","shell.execute_reply":"2022-09-09T06:05:20.868467Z","shell.execute_reply.started":"2022-09-09T06:05:19.797866Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Emotion</th>\n","      <th>File_Path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>surprise</td>\n","      <td>../Datasets/Tess/OAF_Pleasant_surprise/OAF_dab...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>surprise</td>\n","      <td>../Datasets/Tess/OAF_Pleasant_surprise/OAF_mak...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>surprise</td>\n","      <td>../Datasets/Tess/OAF_Pleasant_surprise/OAF_bea...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>surprise</td>\n","      <td>../Datasets/Tess/OAF_Pleasant_surprise/OAF_hal...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>surprise</td>\n","      <td>../Datasets/Tess/OAF_Pleasant_surprise/OAF_pas...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Emotion                                          File_Path\n","0  surprise  ../Datasets/Tess/OAF_Pleasant_surprise/OAF_dab...\n","1  surprise  ../Datasets/Tess/OAF_Pleasant_surprise/OAF_mak...\n","2  surprise  ../Datasets/Tess/OAF_Pleasant_surprise/OAF_bea...\n","3  surprise  ../Datasets/Tess/OAF_Pleasant_surprise/OAF_hal...\n","4  surprise  ../Datasets/Tess/OAF_Pleasant_surprise/OAF_pas..."]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["tess=[]\n","for directory in os.listdir(Tess_Path):\n","    for wav in os.listdir(os.path.join(Tess_Path,directory)):\n","        emotion=wav.partition('.wav')[0].split('_')\n","        if emotion[2]=='ps':\n","            tess.append(('surprise',os.path.join(Tess_Path,directory,wav)))\n","        else:\n","            tess.append((emotion[2],os.path.join(Tess_Path,directory,wav)))\n","Tess_df=pd.DataFrame.from_dict(tess)\n","Tess_df.rename(columns={0:'Emotion',1:'File_Path'},inplace=True)\n","Tess_df.head()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-09-09T06:05:20.873468Z","iopub.status.busy":"2022-09-09T06:05:20.873223Z","iopub.status.idle":"2022-09-09T06:05:20.885293Z","shell.execute_reply":"2022-09-09T06:05:20.884049Z","shell.execute_reply.started":"2022-09-09T06:05:20.873437Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(12162, 2)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["main_df=pd.concat([Crema_df,Ravdess_df,Savee_df,Tess_df],axis=0)\n","main_df.shape"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-09-09T06:05:20.887615Z","iopub.status.busy":"2022-09-09T06:05:20.886978Z","iopub.status.idle":"2022-09-09T06:05:20.900298Z","shell.execute_reply":"2022-09-09T06:05:20.899498Z","shell.execute_reply.started":"2022-09-09T06:05:20.887557Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Emotion</th>\n","      <th>File_Path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>angry</td>\n","      <td>../Datasets/Crema//1072_IOM_ANG_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>fear</td>\n","      <td>../Datasets/Crema//1048_IOM_FEA_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>angry</td>\n","      <td>../Datasets/Crema//1073_IEO_ANG_LO.wav</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>happy</td>\n","      <td>../Datasets/Crema//1012_WSI_HAP_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>happy</td>\n","      <td>../Datasets/Crema//1073_TSI_HAP_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>happy</td>\n","      <td>../Datasets/Crema//1089_TSI_HAP_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>disgust</td>\n","      <td>../Datasets/Crema//1031_MTI_DIS_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>disgust</td>\n","      <td>../Datasets/Crema//1037_ITH_DIS_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>sad</td>\n","      <td>../Datasets/Crema//1075_IEO_SAD_HI.wav</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>disgust</td>\n","      <td>../Datasets/Crema//1051_MTI_DIS_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>sad</td>\n","      <td>../Datasets/Crema//1084_ITS_SAD_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>happy</td>\n","      <td>../Datasets/Crema//1063_TSI_HAP_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>angry</td>\n","      <td>../Datasets/Crema//1035_WSI_ANG_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>fear</td>\n","      <td>../Datasets/Crema//1012_DFA_FEA_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>fear</td>\n","      <td>../Datasets/Crema//1064_IEO_FEA_HI.wav</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Emotion                               File_Path\n","0     angry  ../Datasets/Crema//1072_IOM_ANG_XX.wav\n","1      fear  ../Datasets/Crema//1048_IOM_FEA_XX.wav\n","2     angry  ../Datasets/Crema//1073_IEO_ANG_LO.wav\n","3     happy  ../Datasets/Crema//1012_WSI_HAP_XX.wav\n","4     happy  ../Datasets/Crema//1073_TSI_HAP_XX.wav\n","5     happy  ../Datasets/Crema//1089_TSI_HAP_XX.wav\n","6   disgust  ../Datasets/Crema//1031_MTI_DIS_XX.wav\n","7   disgust  ../Datasets/Crema//1037_ITH_DIS_XX.wav\n","8       sad  ../Datasets/Crema//1075_IEO_SAD_HI.wav\n","9   disgust  ../Datasets/Crema//1051_MTI_DIS_XX.wav\n","10      sad  ../Datasets/Crema//1084_ITS_SAD_XX.wav\n","11    happy  ../Datasets/Crema//1063_TSI_HAP_XX.wav\n","12    angry  ../Datasets/Crema//1035_WSI_ANG_XX.wav\n","13     fear  ../Datasets/Crema//1012_DFA_FEA_XX.wav\n","14     fear  ../Datasets/Crema//1064_IEO_FEA_HI.wav"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["main_df.head(15)"]},{"cell_type":"markdown","metadata":{},"source":["# Importing Data"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# import soundfile as sf\n","\n","# for i, row in tqdm(main_df.iterrows(), total=len(main_df)):\n","#     # data, rate = sf.read(row['File_Path'])\n","\n","#     data, rate = librosa.load(row['File_Path'])\n","#     emo = row['Emotion']\n","\n","#     if rate != CFG.sample_rate:\n","#         try:\n","#             data = librosa.resample(y=data, orig_sr=rate, target_sr=CFG.sample_rate)\n","#         except Exception as e:\n","#             print('Error resampling file:', row['File_Path'])\n","\n","#     sf.write(f'../Datasets/custom/{emo}_{i}.wav', data, CFG.sample_rate, 'PCM_16')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["x = []\n","y = []\n","\n","for dir, subdirs, files in os.walk('../Datasets/custom'):\n","    for file in files:\n","        path = os.path.join(dir, file)\n","        emo = file.split('_')[0]\n","        x.append(path)\n","        y.append(emo)\n","    \n","    \n","dic = {'File_Path': x, 'Emotion': y}\n","main_df = pd.DataFrame(dic)"]},{"cell_type":"markdown","metadata":{},"source":["### Modifying Dataframe"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["10673"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["main_df = main_df[main_df['Emotion'] != 'surprise']\n","len(main_df)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-09-09T06:05:20.902444Z","iopub.status.busy":"2022-09-09T06:05:20.901630Z","iopub.status.idle":"2022-09-09T06:05:21.336535Z","shell.execute_reply":"2022-09-09T06:05:21.335755Z","shell.execute_reply.started":"2022-09-09T06:05:20.902399Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["plt.figure(figsize=(12,6))\n","plt.title('Emotions Counts')\n","emotions=sns.countplot(x='Emotion',data=main_df,palette='Set2')\n","emotions.set_xticklabels(emotions.get_xticklabels(),rotation=45)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Splitting Dataset"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-09-09T06:50:45.795799Z","iopub.status.busy":"2022-09-09T06:50:45.795119Z","iopub.status.idle":"2022-09-09T06:50:45.863012Z","shell.execute_reply":"2022-09-09T06:50:45.862089Z","shell.execute_reply.started":"2022-09-09T06:50:45.795755Z"},"trusted":true},"outputs":[],"source":["\n","def get_splits(df: pd.DataFrame):\n","\n","    # Defines ratios, w.r.t. whole dataset.\n","    ratio_train = 0.7\n","    ratio_val = 0.20\n","    ratio_test = 0.10\n","\n","    # Produces test split.\n","    remaining, test = train_test_split(\n","        df, test_size=ratio_test, stratify=df['Emotion'])\n","\n","    # Adjusts val ratio, w.r.t. remaining dataset.\n","    ratio_remaining = 1 - ratio_test\n","    ratio_val_adjusted = ratio_val / ratio_remaining\n","\n","    # Produces train and val splits.\n","    train, val = train_test_split(\n","        remaining, test_size=ratio_val_adjusted, stratify=remaining['Emotion'])\n","    \n","    return train, val, test "]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["(7470, 2135, 1068)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["train, val, test = get_splits(main_df)\n","len(train), len(val), len(test)"]},{"cell_type":"markdown","metadata":{},"source":["# New stuff"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Compute Spectrogram from audio\n","\n","def Audio2Mfcc(audio):\n","\n","    stfts = tf.signal.stft(audio, frame_length=CFG.n_fft, frame_step=CFG.hop_len,\n","                        fft_length=CFG.n_fft)\n","    spectrograms = tf.abs(stfts)\n","\n","\n","    # Warp the linear scale spectrograms into the mel-scale.\n","    num_spectrogram_bins = stfts.shape[-1]\n","\n","    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n","    64, # Can be changed\n","    num_spectrogram_bins, \n","    CFG.sample_rate, \n","    CFG.fmin,\n","    CFG.fmax)\n","\n","    mel_spectrograms = tf.tensordot(\n","    spectrograms, linear_to_mel_weight_matrix, 1)\n","\n","    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(\n","    linear_to_mel_weight_matrix.shape[-1:]))\n","\n","    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n","    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n","\n","    # Compute MFCCs from log_mel_spectrograms and take the first 13.\n","    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(\n","    log_mel_spectrograms)[..., :32]\n","    return mfccs\n","\n","\n","def Audio2Spec(audio,spec_shape=[256, 128],sr=16000,nfft=2048,window=2048,fmin=20,fmax=8000):\n","    spec_time = spec_shape[0]\n","    spec_freq = spec_shape[1]\n","    audio_len = tf.shape(audio)[0]\n","    hop_length = tf.cast((audio_len // (spec_time - 1)), tf.int32) # compute hop_length to keep desired spec_shape\n","    spec = tfio.audio.spectrogram(audio, nfft=nfft, window=window, stride=hop_length) # convert to spectrogram\n","    mel_spec = tfio.audio.melscale(spec, rate=sr, mels=spec_freq, fmin=fmin, fmax=fmax) # transform to melscale\n","    db_mel_spec = tfio.audio.dbscale(mel_spec, top_db=80) # from power to db (log10) scale\n","    if tf.shape(db_mel_spec)[0] > spec_time:  # check if we have desiered shape\n","        db_mel_spec = db_mel_spec[:spec_time,:]\n","    db_mel_spec = tf.reshape(db_mel_spec, spec_shape)\n","\n","\n","    return db_mel_spec\n","\n","# Convert spectrogram (H,W) to image (H,W,1)\n","def Spec2Img(spec, num_channels=1):\n","    # 1 channel image\n","    img = spec[..., tf.newaxis]\n","    # Copy same image across channel axis\n","    if num_channels>1:\n","        img = tf.tile(img, [1, 1, num_channels])\n","    return img"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# Decode audio from wav\n","def decode_audio(data):\n","\n","    # Decode\n","    audio, sr = tf.audio.decode_wav(data)\n","\n","\n","    audio = tf.cast(audio,tf.float32)\n","    # Normalization\n","    if CFG.normalize:\n","        audio = Normalize(audio)\n","    return audio\n","\n","skippedFiles = []\n","# Read tfrecord data & parse it & do augmentation\n","def save_img(file, label, set, count, augment=True, return_id=False, return_label=True, target_len=CFG.audio_len, spec_shape=CFG.spec_shape):\n","    \n","    audio = tf.io.read_file(file)\n","\n","\n","    # # Decoding\n","    try:\n","        audio = decode_audio(audio)  # decode audio from .wav\n","    except Exception as e:\n","        skippedFiles.append(file)\n","        return\n","\n","\n","    try:\n","        audio = tf.squeeze(audio, axis=[-1]) # remove channel dimension\n","    except:\n","        skippedFiles.append(file)\n","        return\n","    \n","\n","    # audio = tf.cast(audio, tf.float32) / 32768.0\n","\n","    # Trim Audio\n","    audio = TrimAudio(audio)\n","    # Crop or Pad audio to keep a fixed length\n","    audio = CropOrPad(audio, target_len)\n","    if augment:\n","        # Apply AudioAug\n","        audio = AudioAug(audio)\n","    # Compute Spectrogram\n","    # spec = Audio2Spec(audio, spec_shape=spec_shape)\n","    spec = Audio2Mfcc(audio)\n","    if augment:\n","        # Apply SpecAug\n","        spec = SpecAug(spec)\n","    # Spectrogram (H, W) to Image (H, W, C)\n","    img = Spec2Img(spec, num_channels=1) \n","    # Clip & Reshape\n","    img = tf.clip_by_value(img, 0, 1) if CFG.clip else img\n","    # print(img)\n","    # img = tf.reshape(img, [..., 1])\n","\n","    img = img.numpy().transpose()[0]\n","    cm = plt.cm.get_cmap('jet')\n","    plt.tight_layout();\n","    plt.grid(False)\n","\n","    if augment:\n","        plt.imsave(f'./features/images/new/{set}/{label}_{count}_augged.png', img, cmap=cm)\n","    else:\n","        plt.imsave(f'./features/images/new/{set}/{label}_{count}.png', img, cmap=cm)\n","    \n","    plt.cla()\n","    plt.close(\"all\")\n","\n","    # Save image\n","    # img = tf.image.convert_image_dtype(img, tf.uint8)\n","    # img = tf.image.encode_png(img)\n","    # tf.io.write_file(f'./features/images/new/{set}/{label}_{count}.png', img)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/7470 [00:00<?, ?it/s]2022-09-24 08:24:26.641565: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n","2022-09-24 08:24:26.641595: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: archlinux\n","2022-09-24 08:24:26.641600: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: archlinux\n","2022-09-24 08:24:26.641770: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 515.76.0\n","2022-09-24 08:24:26.641790: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.65.1\n","2022-09-24 08:24:26.641795: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 515.65.1 does not match DSO version 515.76.0 -- cannot find working devices in this configuration\n","2022-09-24 08:24:26.642610: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","100%|| 7470/7470 [07:41<00:00, 16.20it/s]\n"]}],"source":["for i, row in tqdm(train.iterrows(), total=len(train)):\n","        save_img(row['File_Path'], row['Emotion'], 'train', i)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|| 7470/7470 [07:23<00:00, 16.84it/s] \n"]}],"source":["for i, row in tqdm(train.iterrows(), total=len(train)):\n","        save_img(row['File_Path'], row['Emotion'], 'train', i, augment=False)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|| 2135/2135 [02:12<00:00, 16.15it/s]\n"]}],"source":["for i, row in tqdm(val.iterrows(), total=len(val)):\n","        save_img(row['File_Path'], row['Emotion'], 'val', i, augment=False)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|| 1068/1068 [01:03<00:00, 16.71it/s]\n"]}],"source":["for i, row in tqdm(test.iterrows(), total=len(test)):\n","        save_img(row['File_Path'], row['Emotion'], 'test', i, augment=False)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["['../Datasets/custom/fear_176.wav', '../Datasets/custom/fear_176.wav']"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["sorted(skippedFiles)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"}}},"nbformat":4,"nbformat_minor":4}
